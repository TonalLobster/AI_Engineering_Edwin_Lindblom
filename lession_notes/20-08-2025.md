A feature(oberoende variabel) in linear regression is could for example be how many years someone has ha experience in a field(x axel).

y axeln lÃ¤gger man nÃ¥got som kallas fÃ¶r "Target". Detta kan till exempel vara hur mycket man tjÃ¤nar nÃ¤r man fick frÃ¥gan(ex) "Hur mycket tjÃ¤nar du?".
VarfÃ¶r det heter target Ã¤r att man efter ett visst antal features sÃ¥ ska man kunna gissa hur mycket dom har i lÃ¶n beroende pÃ¥ hur mycket erfarenhet dom har.



Simple linear regression har bara tex en linje, dvs en feature och en target.

"n features(more than one)" sÃ¥ anvÃ¤nds "n dimensional plane". alltsÃ¥ flera oberoende variabler.


3b1b(3 blue 1 brown) pÃ¥ youtube. bra att kolla pÃ¥.

ISLP
introduction to statistical learning with python. Kan va bra fÃ¶r o lÃ¤ra sig. Dock rÃ¤tt svÃ¥rt.


"""
Text relaterat till multiple_linear_regression.png:

En 3d modell och lite info jag fick frÃ¥n copilot!

    ğŸ§­ **Axlarna i modellen:**
    - **Xâ‚ â€“ Ã…lder** (t.ex. 20â€“40 Ã¥r)
    - **Xâ‚‚ â€“ Erfarenhet** (t.ex. 2â€“10 Ã¥r)
    - **Y â€“ MÃ¥nadslÃ¶n** (t.ex. 30 000â€“70 000 kr)

    ğŸ“Š **Vad du ser:**
    - De **rÃ¶da punkterna** visar verkliga personer med olika kombinationer av Ã¥lder och erfarenhet, samt vilken lÃ¶n de har.
    - Det **fÃ¤rgade planet** Ã¤r den linjÃ¤ra modellen som fÃ¶rsÃ¶ker fÃ¶rutsÃ¤ga lÃ¶nen baserat pÃ¥ Ã¥lder och erfarenhet.
    - Ju mer spridda punkterna Ã¤r frÃ¥n planet, desto mer variation finns i verkligheten som modellen inte fÃ¥ngar helt.

    Det hÃ¤r Ã¤r ett perfekt exempel pÃ¥ hur en **multiple linear regression** fungerar i praktiken â€“ Ã¤ven med varierande data. Modellen fÃ¶rsÃ¶ker hitta det bÃ¤sta planet som minimerar avstÃ¥ndet till alla punkter.

"""

nÃ¤r man trÃ¤nar upp en modell sÃ¥ anvÃ¤nder man sig utav bÃ¥de x och y label, dvs typ erfarenhet och mÃ¥nadslÃ¶n. fÃ¶r att sedan kunna testa datan man fÃ¥tt ut.

A feature(oberoende variabel) in linear regression is could for example be how many years someone has ha experience in a field(x axel).

y axeln lägger man något som kallas för "Target". Detta kan till exempel vara hur mycket man tjänar när man fick frågan(ex) "Hur mycket tjänar du?".
Varför det heter target är att man efter ett visst antal features så ska man kunna gissa hur mycket dom har i lön beroende på hur mycket erfarenhet dom har.



Simple linear regression har bara tex en linje, dvs en feature och en target.

"n features(more than one)" så används "n dimensional plane". alltså flera oberoende variabler.


3b1b(3 blue 1 brown) på youtube. bra att kolla på.

ISLP
introduction to statistical learning with python. Kan va bra för o lära sig. Dock rätt svårt.


"""
Text relaterat till multiple_linear_regression.png:

En 3d modell och lite info jag fick från copilot!

    🧭 **Axlarna i modellen:**
    - **X₁ – Ålder** (t.ex. 20–40 år)
    - **X₂ – Erfarenhet** (t.ex. 2–10 år)
    - **Y – Månadslön** (t.ex. 30 000–70 000 kr)

    📊 **Vad du ser:**
    - De **röda punkterna** visar verkliga personer med olika kombinationer av ålder och erfarenhet, samt vilken lön de har.
    - Det **färgade planet** är den linjära modellen som försöker förutsäga lönen baserat på ålder och erfarenhet.
    - Ju mer spridda punkterna är från planet, desto mer variation finns i verkligheten som modellen inte fångar helt.

    Det här är ett perfekt exempel på hur en **multiple linear regression** fungerar i praktiken – även med varierande data. Modellen försöker hitta det bästa planet som minimerar avståndet till alla punkter.

"""

när man tränar upp en modell så använder man sig utav både x och y label, dvs typ erfarenhet och månadslön. för att sedan kunna testa datan man fått ut.
